{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow 2 Complete Project Workflow in Amazon SageMaker\n",
    "### Model Training\n",
    "    \n",
    "1. [SKLearn Linear Regression](#LogitRegression)\n",
    "2. [SageMaker Linear Learner](#LinearLearner)\n",
    "3. [SageMaker XGBoost](#XGBoost)\n",
    "4. [Local Mode training](#LocalModeTraining)\n",
    "5. [SageMaker hosted training](#SageMakerHostedTraining)\n",
    "6. [Automatic model tuning](#AutomaticModelTuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll import the variables stored from previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  SKLearn Linear Regression <a class=\"anchor\" id=\"LogitRegression\">\n",
    "\n",
    "To start out with, we're going to train a scikit-learn logistic regression model like you'd normally do on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# We'll stitch the transformed dataset back together\n",
    "# since KFold takes care of splitting for us\n",
    "x_1 = np.load('./data/train/x_train.npy')\n",
    "x_2 = np.load('./data/test/x_test.npy')\n",
    "y_1 = np.load('./data/train/y_train.npy')\n",
    "y_2 = np.load('./data/test/y_test.npy')\n",
    "x = np.concatenate((x_1, x_2))\n",
    "y = np.concatenate((y_1, y_2))\n",
    "\n",
    "# Vanilla linear regression\n",
    "l_regression = linear_model.LinearRegression()\n",
    "kf = KFold(n_splits=10)\n",
    "min_max_scaler = MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "scores = cross_val_score(l_regression, x, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "print(\"MSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
    "scores_map = {}\n",
    "scores_map['LinearRegression'] = scores\n",
    "\n",
    "# Linear regression with L2 regularization\n",
    "l_ridge = linear_model.Ridge()\n",
    "scores = cross_val_score(l_ridge, x, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "scores_map['Ridge'] = scores\n",
    "print(\"MSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Polinomial regression with L2 regularization with degree for the best fit\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "model = make_pipeline(PolynomialFeatures(degree=3), linear_model.Ridge())\n",
    "scores = cross_val_score(model, x, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "scores_map['PolyRidge'] = scores\n",
    "print(\"MSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Liner Regression with and without L2 regularization does not make significant difference is MSE score. However polynomial regression with degree=3 has a better MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  SageMaker Linear Learner <a class=\"anchor\" id=\"LinearLearner\">\n",
    "\n",
    "Next we'll try SageMaker Linear Learner. The SageMaker Linear Learner algorithm provides a significant increase in speed over naive hyperparameter optimization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we convert our numpy data into `RecordSet` objects. Although Linear Learner training supports CSV, most Amazon SageMaker algorithms work best when you use the optimized protobuf recordIO data format for training. Using this format allows you to take advantage of Pipe mode. In Pipe mode, your training job streams data directly from Amazon Simple Storage Service (Amazon S3). Streaming can provide faster start times for training jobs and better throughput. You can find more about this format [here](https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create the Linear Learner model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "role=sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_location = f's3://{bucket}/{s3_prefix}/linearlearner'\n",
    "\n",
    "linear_learner = sagemaker.LinearLearner(role=role,\n",
    "                                         train_instance_count=1,\n",
    "                                         train_instance_type='ml.c4.xlarge',\n",
    "                                         predictor_type='regressor',\n",
    "                                         output_path=output_location,\n",
    "                                         sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('./data/train/x_train.npy')\n",
    "y_train = np.load('./data/train/y_train.npy')\n",
    "x_test = np.load('./data/test/x_test.npy')\n",
    "y_test = np.load('./data/test/y_test.npy')\n",
    "\n",
    "training_data_recordset = linear_learner.record_set(train=x_train.astype('float32'),\n",
    "                                                    labels=y_train.astype('float32'),\n",
    "                                                    channel='train')\n",
    "testing_data_recordset = linear_learner.record_set(train=x_test.astype('float32'),\n",
    "                                                   labels=y_test.astype('float32'),\n",
    "                                                   channel='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we fit the model on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_learner.fit([training_data_recordset, testing_data_recordset])\n",
    "linear_model_data = linear_learner.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a test MSE of 0.1, the Linear Learner model beat our more manual exploration using scikit-learn's linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  SageMaker XGBoost <a class=\"anchor\" id=\"XGBoost\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun, let's try our hand at XGBoost. But first, we need to merge the features and labels and save the data as CSV to our local disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "x_train = np.load('./data/train/x_train.npy')\n",
    "x_test = np.load('./data/test/x_test.npy')\n",
    "y_train = np.load('./data/train/y_train.npy')\n",
    "y_test = np.load('./data/test/y_test.npy')\n",
    "\n",
    "train_df = pd.DataFrame(data=x_train)\n",
    "train_df['target'] = y_train\n",
    "first_col = train_df.pop('target')\n",
    "train_df.insert(0, 'target', first_col)\n",
    "\n",
    "test_df = pd.DataFrame(data=x_test)\n",
    "test_df['target'] = y_test\n",
    "first_col = test_df.pop('target')\n",
    "test_df.insert(0, 'target', first_col)\n",
    "\n",
    "train_df.to_csv('./data/train/train.csv', header=False, index=False)\n",
    "test_df.to_csv('./data/test/validation.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then upload those CSVs to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from sagemaker.session import s3_input\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(s3_prefix, 'data/train/train.csv')).upload_file('./data/train/train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(s3_prefix, 'data/validation/validation.csv')).upload_file('./data/test/validation.csv')\n",
    "s3_input_train = f's3://{bucket}/{s3_prefix}/data/train/train.csv'\n",
    "s3_input_test = f's3://{bucket}/{s3_prefix}/data/validation/validation.csv'\n",
    "s3_input_train = s3_input(s3_input_train, content_type='csv')\n",
    "s3_input_test = s3_input(s3_input_test, content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the XGBoost model and pass in the required hyperparameter. Details of hyperparamters can be found [here](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "container = get_image_uri(sess.boto_region_name, 'xgboost', repo_version='1.0-1')\n",
    "output_location = f's3://{bucket}/{s3_prefix}/xgboost'\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path=output_location,\n",
    "                                    sagemaker_session=sess)\n",
    "\n",
    "xgb.set_hyperparameters(num_round=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we fit the XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A test RMSE of 0.19. Not bad but not as good as the Linear Learner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Experiment tracking <a class=\"anchor\" id=\"Experiment\">\n",
    "SageMaker experiments can track all the model training iterations. Experiments are a great way to organize your data science work. You can create experiments to organize all your model development work for : [1] a business use case you are addressing (e.g. create experiment named “customer churn prediction”), or [2] a data science team that owns the experiment (e.g. create experiment named “marketing analytics experiment”), or [3] a specific data science and ML project. Think of it as a “folder” for organizing your “files”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install sagemaker-experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smexperiments.experiment import Experiment\n",
    "from time import gmtime, strftime\n",
    "import boto3\n",
    "\n",
    "sm = boto3.client('sagemaker')\n",
    "\n",
    "experiment = Experiment.create(\n",
    "    experiment_name=\"boston-housing-regression-{}\".format(strftime(\"%d-%H-%M-%S\", gmtime())), \n",
    "    description=\"Boston housing price estimation.\", \n",
    "    sagemaker_boto_client=sm)\n",
    "\n",
    "experiment_config={\n",
    "    \"ExperimentName\": experiment.experiment_name,\n",
    "    \"TrialComponentDisplayName\": \"Training\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Local Mode training<a class=\"anchor\" id=\"LocalModeTraining\">\n",
    "    \n",
    "Before we begin, we'll pull the original Boston Housing dataset as the TensorFlow example we're about to run uses all features in its architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.datasets import boston_housing\n",
    "import os\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n",
    "\n",
    "# Clean up local train and test folders\n",
    "filelist = [ f for f in os.listdir('./data/train') if (f.endswith(\".npy\") or f.endswith(\".csv\"))]\n",
    "for f in filelist:\n",
    "    os.remove(os.path.join('./data/train', f))\n",
    "    \n",
    "filelist = [ f for f in os.listdir('./data/test') if (f.endswith(\".npy\") or f.endswith(\".csv\"))]\n",
    "for f in filelist:\n",
    "    os.remove(os.path.join('./data/test', f))\n",
    "    \n",
    "# Clean up S3\n",
    "remove_train = 's3://{}/{}/data/train/'.format(bucket, s3_prefix)\n",
    "remove_test = 's3://{}/{}/data/test/'.format(bucket, s3_prefix)\n",
    "!aws s3 rm {remove_train} --recursive\n",
    "!aws s3 rm {remove_test} --recursive\n",
    "\n",
    "# Save the original Boston Housing dataset\n",
    "raw_dir = os.path.join(os.getcwd(), 'data/raw')\n",
    "np.save(os.path.join(raw_dir, 'x_train.npy'), x_train)\n",
    "np.save(os.path.join(raw_dir, 'x_test.npy'), x_test)\n",
    "np.save(os.path.join(train_dir, 'x_train.npy'), x_train)\n",
    "np.save(os.path.join(test_dir, 'x_test.npy'), x_test)\n",
    "np.save(os.path.join(train_dir, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(test_dir, 'y_test.npy'), y_test)\n",
    "\n",
    "# Upload fresh to S3\n",
    "rawdata_s3_prefix = '{}/data/raw'.format(s3_prefix)\n",
    "raw_s3 = sess.upload_data(path='./data/raw/', key_prefix=rawdata_s3_prefix)\n",
    "output_destination = 's3://{}/{}/data'.format(bucket, s3_prefix)\n",
    "x_train_in_s3 = '{}/train/x_train.npy'.format(output_destination)\n",
    "x_test_in_s3 = '{}/test/x_test.npy'.format(output_destination)\n",
    "y_train_in_s3 = '{}/train/y_train.npy'.format(output_destination)\n",
    "y_test_in_s3 = '{}/test/y_test.npy'.format(output_destination)\n",
    "!aws s3 cp ./data/train/x_train.npy {x_train_in_s3}\n",
    "!aws s3 cp ./data/test/x_test.npy {x_test_in_s3}\n",
    "!aws s3 cp ./data/train/y_train.npy {y_train_in_s3}\n",
    "!aws s3 cp ./data/test/y_test.npy {y_test_in_s3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local Mode in Amazon SageMaker is a convenient way to make sure your code is working locally as expected before moving on to full scale, hosted training in a separate, more powerful SageMaker-managed cluster. To train in Local Mode, it is necessary to have docker-compose or nvidia-docker-compose (for GPU instances) installed. Running the following commands will install docker-compose or nvidia-docker-compose, and configure the notebook environment for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://raw.githubusercontent.com/aws-samples/amazon-sagemaker-script-mode/master/local_mode_setup.sh\n",
    "!wget -q https://raw.githubusercontent.com/aws-samples/amazon-sagemaker-script-mode/master/daemon.json    \n",
    "!/bin/bash ./local_mode_setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll set up a TensorFlow Estimator for Local Mode training. Key parameters for the Estimator include:\n",
    "\n",
    "- `train_instance_type`: the kind of hardware on which training will run. In the case of Local Mode, we simply set this parameter to `local` to invoke Local Mode training on the CPU, or to `local_gpu` if the instance has a GPU. \n",
    "- `git_config`:  to make sure training scripts are source controlled for coordinated, shared use by a team, the Estimator can pull in the code from a Git repository rather than local directories.  \n",
    "- Other parameters of note: the algorithm’s hyperparameters, which are passed in as a dictionary, and a Boolean parameter indicating that we are using Script Mode. \n",
    "\n",
    "Recall that we are using Local Mode here mainly to make sure our code is working. Accordingly, instead of performing a full cycle of training with many epochs (passes over the full dataset), we'll train only for a small number of epochs just to confirm the code is working properly and avoid wasting full-scale training time unnecessarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "git_config = {'repo': 'https://github.com/aws-samples/amazon-sagemaker-script-mode', \n",
    "              'branch': 'master'}\n",
    "\n",
    "model_dir = '/opt/ml/model'\n",
    "train_instance_type = 'local'\n",
    "hyperparameters = {'epochs': 5, 'batch_size': 128, 'learning_rate': 0.01}\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "local_estimator_parameters = {'git_config': git_config,\n",
    "                             'source_dir': 'tf-2-workflow/train_model',\n",
    "                             'entry_point':'train.py',\n",
    "                             'model_dir': model_dir,\n",
    "                             'train_instance_type' : train_instance_type,\n",
    "                             'train_instance_count': 1,\n",
    "                             'hyperparameters': hyperparameters,\n",
    "                             'role' : role,\n",
    "                             'base_job_name':'tf-2-workflow',\n",
    "                             'framework_version':'2.1',\n",
    "                             'py_version':'py3',\n",
    "                             'script_mode':True}\n",
    "\n",
    "local_estimator = TensorFlow(**local_estimator_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit` method call below starts the Local Mode training job.  Metrics for training will be logged below the code, inside the notebook cell.  You should observe the validation loss decrease substantially over the five epochs, with no training errors, which is a good indication that our training code is working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {'train': f'file://{train_dir}',\n",
    "          'test': f'file://{test_dir}'}\n",
    "\n",
    "local_estimator.fit(inputs)\n",
    "\n",
    "local_model_data = local_estimator.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  SageMaker hosted training <a class=\"anchor\" id=\"SageMakerHostedTraining\">\n",
    "\n",
    "Now that we've confirmed our code is working locally, we can move on to use SageMaker's hosted training functionality. Hosted training is preferred for doing actual training, especially large-scale, distributed training.  Unlike Local Mode training, for hosted training the actual training itself occurs not on the notebook instance, but on a separate cluster of machines managed by SageMaker.  Before starting hosted training, the data must be in S3, or an EFS or FSx for Lustre file system. We'll upload to S3 now, and confirm the upload was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_s3_prefix = 's3://{}/{}/data/train'.format(bucket, s3_prefix)\n",
    "testdata_s3_prefix = 's3://{}/{}/data/test'.format(bucket, s3_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {'train':traindata_s3_prefix, 'test': testdata_s3_prefix}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to set up an Estimator object for hosted training. It is similar to the Local Mode Estimator, except the `train_instance_type` has been set to a SageMaker ML instance type instead of `local` for Local Mode. Also, since we know our code is working now, we'll train for a larger number of epochs with the expectation that model training will converge to an improved, lower validation loss.\n",
    "\n",
    "With these two changes, we simply call `fit` to start the actual hosted training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instance_type = 'ml.c5.xlarge'\n",
    "hyperparameters = {'epochs': 30, 'batch_size': 128, 'learning_rate': 0.01}\n",
    "\n",
    "estimator_parameters = {'git_config': git_config,\n",
    "                        'source_dir': 'tf-2-workflow/train_model',\n",
    "                        'entry_point':'train.py',\n",
    "                        'model_dir': model_dir,\n",
    "                        'train_instance_type' : train_instance_type,\n",
    "                        'train_instance_count': 1,\n",
    "                        'hyperparameters': hyperparameters,\n",
    "                        'role' : sagemaker.get_execution_role(),\n",
    "                        'base_job_name':'tf-2-workflow',\n",
    "                        'framework_version':'2.1',\n",
    "                        'py_version':'py3',\n",
    "                        'script_mode':True}\n",
    "\n",
    "estimator = TensorFlow(**estimator_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After starting the hosted training job with the `fit` method call below, you should observe the training converge over the longer number of epochs to a validation loss that is considerably lower than that which was achieved in the shorter Local Mode training job.  Can we do better? We'll look into a way to do so in the **Automatic Model Tuning** section below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(inputs, experiment_config=experiment_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that training finished, we can use SageMaker Experiments to examine the results and see how it compares to other training jobs within the experiment. Right now this is the only job captured in Experiments, but let's take a look anyway to see what data it stores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "\n",
    "trial_component_analytics = ExperimentAnalytics(\n",
    "    sagemaker_session=sess, \n",
    "    experiment_name=experiment.experiment_name\n",
    ")\n",
    "\n",
    "experiments_df = trial_component_analytics.dataframe()\n",
    "experiments_df.columns\n",
    "cols = ['TrialComponentName', 'val_loss_EVAL - Last']\n",
    "experiments_df = experiments_df[cols]\n",
    "experiments_df.sort_values(by='val_loss_EVAL - Last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_name = estimator.latest_training_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the Local Mode training, hosted training produces a model saved in S3 that we can retrieve.  This is an example of the modularity of SageMaker: having trained the model in SageMaker, you can now take the model out of SageMaker and run it anywhere else.  Alternatively, you can deploy the model into a production-ready environment using SageMaker's hosted endpoints functionality, as shown in the **SageMaker hosted endpoint** section below.\n",
    "\n",
    "Retrieving the model from S3 is very easy:  the hosted training estimator you created above stores a reference to the model's location in S3.  You simply copy the model from S3 using the estimator's `model_data` property and unzip it to inspect the contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp {estimator.model_data} ./model/model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unzipped archive should include the assets required by TensorFlow Serving to load the model and serve it, including a .pb file:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvzf ./model/model.tar.gz -C ./model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Model Tuning <a class=\"anchor\" id=\"AutomaticModelTuning\">\n",
    "\n",
    "So far we have simply run one Local Mode training job and one Hosted Training job without any real attempt to tune hyperparameters to produce a better model, other than increasing the number of epochs.  Selecting the right hyperparameter values to train your model can be difficult, and typically is very time consuming if done manually. The right combination of hyperparameters is dependent on your data and algorithm; some algorithms have many different hyperparameters that can be tweaked; some are very sensitive to the hyperparameter values selected; and most have a non-linear relationship between model fit and hyperparameter values.  SageMaker Automatic Model Tuning helps automate the hyperparameter tuning process:  it runs multiple training jobs with different hyperparameter combinations to find the set with the best model performance.\n",
    "\n",
    "We begin by specifying the hyperparameters we wish to tune, and the range of values over which to tune each one.  We also must specify an objective metric to be optimized:  in this use case, we'd like to minimize the validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "  'learning_rate': ContinuousParameter(0.001, 0.2, scaling_type=\"Logarithmic\"),\n",
    "  'epochs': IntegerParameter(10, 50),\n",
    "  'batch_size': IntegerParameter(64, 256),\n",
    "}\n",
    "\n",
    "metric_definitions = [{'Name': 'loss',\n",
    "                       'Regex': ' loss: ([0-9\\\\.]+)'},\n",
    "                     {'Name': 'val_loss',\n",
    "                       'Regex': ' val_loss: ([0-9\\\\.]+)'}]\n",
    "\n",
    "objective_metric_name = 'val_loss'\n",
    "objective_type = 'Minimize'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we specify a HyperparameterTuner object that takes the above definitions as parameters.  Each tuning job must be given a budget:  a maximum number of training jobs.  A tuning job will complete after that many training jobs have been executed.  \n",
    "\n",
    "We also can specify how much parallelism to employ, in this case five jobs, meaning that the tuning job will complete after three series of five jobs in parallel have completed.  For the default Bayesian Optimization tuning strategy used here, the tuning search is informed by the results of previous groups of training jobs, so we don't run all of the jobs in parallel, but rather divide the jobs into groups of parallel jobs.  There is a trade-off: using more parallel jobs will finish tuning sooner, but likely will sacrifice tuning search accuracy. \n",
    "\n",
    "Now we can launch a hyperparameter tuning job by calling the `fit` method of the HyperparameterTuner object.  The tuning job may take around 10 minutes to finish.  While you're waiting, the status of the tuning job, including metadata and results for invidual training jobs within the tuning job, can be checked in the SageMaker console in the **Hyperparameter tuning jobs** panel.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "tuner_parameters = {'estimator':estimator,\n",
    "                    'objective_metric_name':objective_metric_name,\n",
    "                    'hyperparameter_ranges':hyperparameter_ranges,\n",
    "                    'metric_definitions':metric_definitions,\n",
    "                    'max_jobs':4,\n",
    "                    'max_parallel_jobs':2,\n",
    "                    'objective_type':objective_type}\n",
    "\n",
    "tuner = HyperparameterTuner(**tuner_parameters)\n",
    "\n",
    "tuning_job_name = \"tf-2-workflow-{}\".format(strftime(\"%d-%H-%M-%S\", gmtime()))\n",
    "tuner.fit(inputs, job_name=tuning_job_name)\n",
    "tuner.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the tuning job is finished, we can use the `HyperparameterTuningJobAnalytics` object from the SageMaker Python SDK to list the top 5 tuning jobs with the best performance. Although the results vary from tuning job to tuning job, the best validation loss from the tuning job (under the FinalObjectiveValue column) likely will be substantially lower than the validation loss from the hosted training job above, where we did not perform any tuning other than manually increasing the number of epochs once.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tuner_metrics = sagemaker.HyperparameterTuningJobAnalytics(tuning_job_name)\n",
    "tuner_metrics.dataframe().sort_values(['FinalObjectiveValue'], ascending=True).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total training time and training jobs status can be checked with the following lines of code. Because automatic early stopping is by default off, all the training jobs should be completed normally.  For an example of a more in-depth analysis of a tuning job, see the SageMaker official sample [HPO_Analyze_TuningJob_Results.ipynb](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/hyperparameter_tuning/analyze_results/HPO_Analyze_TuningJob_Results.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = tuner_metrics.dataframe()['TrainingElapsedTimeSeconds'].sum() / 3600\n",
    "print(\"The total training time is {:.2f} hours\".format(total_time))\n",
    "tuner_metrics.dataframe()['TrainingJobStatus'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the training artifacts created in this notebook in downstream notebooks for model deployment. Store them here for later retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store local_model_data\n",
    "%store role\n",
    "%store training_job_name\n",
    "%store estimator_parameters\n",
    "%store tuning_job_name\n",
    "%store inputs\n",
    "%store x_test\n",
    "%store y_test\n",
    "\n",
    "# Remove estimator key as it's not serializable.\n",
    "if 'estimator' in tuner_parameters: tuner_parameters.__delitem__('estimator')\n",
    "%store tuner_parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
